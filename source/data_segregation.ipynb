{"cells":[{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"-cw3cw33WEvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moc3SHp-k2Ip"},"outputs":[],"source":["import logging\n","import tempfile\n","import pandas as pd\n","import os\n","import wandb\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBZ3IMiY9aD9"},"outputs":[],"source":["# Login to Weights & Biases\n","!wandb login --relogin"]},{"cell_type":"markdown","metadata":{"id":"Nh8AgArRwDyI"},"source":["##1.3 Data segregation\n"]},{"cell_type":"code","source":["# global variables\n","\n","# ratio used to split train and test data\n","test_size = 0.30\n","\n","# seed used to reproduce purposes\n","seed = 41\n","\n","# reference (column) to stratify the data\n","stratify = \"Outcome\"\n","\n","# name of the input artifact\n","artifact_input_name = \"diabetes_nn/preprocessed_data.csv:latest\"\n","\n","# type of the artifact\n","artifact_type = \"segregated_data\""],"metadata":{"id":"mjocVuhAL5Mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure logging\n","logging.basicConfig(level=logging.INFO,\n","                    format=\"%(asctime)s %(message)s\",\n","                    datefmt='%d-%m-%Y %H:%M:%S')\n","\n","# reference for a logging obj\n","logger = logging.getLogger()\n","\n","# initiate wandb project\n","run = wandb.init(project=\"diabetes_nn\", job_type=\"split_data\")\n","\n","logger.info(\"Downloading and reading artifact\")\n","artifact = run.use_artifact(artifact_input_name)\n","artifact_path = artifact.file()\n","df = pd.read_csv(artifact_path)\n","\n","# Split firstly in train/test, then we further divide the dataset to train and validation\n","logger.info(\"Splitting data into train and test\")\n","splits = {}\n","\n","splits[\"train\"], splits[\"test\"] = train_test_split(df,\n","                                                   test_size=test_size,\n","                                                   random_state=seed,\n","                                                   stratify=df[stratify])\n","\n","# Save the artifacts. We use a temporary directory so we do not leave any trace behind\n","with tempfile.TemporaryDirectory() as tmp_dir:\n","\n","    for split, df in splits.items():\n","\n","        # Make the artifact name from the name of the split plus the provided root\n","        artifact_name = f\"{split}.csv\"\n","\n","        # Get the path on disk within the temp directory\n","        temp_path = os.path.join(tmp_dir, artifact_name)\n","\n","        logger.info(f\"Uploading the {split} dataset to {artifact_name}\")\n","\n","        # Save then upload to W&B\n","        df.to_csv(temp_path,index=False)\n","\n","        artifact = wandb.Artifact(name=artifact_name,\n","                                  type=artifact_type,\n","                                  description=f\"{split} split of dataset {artifact_input_name}\",\n","        )\n","        artifact.add_file(temp_path)\n","\n","        logger.info(\"Logging artifact\")\n","        run.log_artifact(artifact)\n","\n","        # This waits for the artifact to be uploaded to W&B. If you\n","        # do not add this, the temp directory might be removed before\n","        # W&B had a chance to upload the datasets, and the upload\n","        # might fail\n","        artifact.wait()"],"metadata":{"id":"_KnIFLXFMHde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# close the run\n","# waiting a while after run the previous cell before execute this\n","run.finish()"],"metadata":{"id":"_JniwpQFMKuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"b24jA0beOsRp"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"data_segregation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}